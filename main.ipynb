{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev. set missing data in 4804 samples out of 6863\n",
      "Test set missing data in 0 samples out of 1716\n",
      "\n",
      "Performing Bayesian search for hyper-parameters optimization, after dropping samples with missing data\n",
      "100%|██████████| 200/200 [01:53<00:00,  1.76trial/s, best loss: -0.7359861507326946]\n",
      "Re-fitting the model with the best hyper-parameter values found: {'depth': 7.0, 'learning_rate': 0.0780758056653478}\n",
      "Training: Log loss=0.207106656849455   ROC AUC=0.8259163299813707\n",
      "Validation: Log loss=0.5791470230273179   ROC AUC=0.7359861507326946\n",
      "\n",
      "Performing Bayesian search for hyper-parameters optimization, with missing data replaced with mean imputer\n",
      "100%|██████████| 200/200 [02:59<00:00,  1.12trial/s, best loss: -0.7504848876676903]\n",
      "Re-fitting the model with the best hyper-parameter values found: {'depth': 6.0, 'learning_rate': 0.09309792048060711}\n",
      "Training: Log loss=0.2729518877810549   ROC AUC=0.9076907266619518\n",
      "Validation: Log loss=0.42857099275069893   ROC AUC=0.7504848876676903\n",
      "\n",
      "Performing Bayesian search for hyper-parameters optimization, with missing data replaced with iterative imputer\n",
      "100%|██████████| 200/200 [02:57<00:00,  1.13trial/s, best loss: -0.7470671030655676]\n",
      "Re-fitting the model with the best hyper-parameter values found: {'depth': 6.0, 'learning_rate': 0.05434741145879128}\n",
      "Training: Log loss=0.32233294293621406   ROC AUC=0.8780509635785007\n",
      "Validation: Log loss=0.4295688197565578   ROC AUC=0.7470671030655676\n",
      "\n",
      "Performing Bayesian search for hyper-parameters optimization, without replacement of missing data\n",
      "100%|██████████| 200/200 [02:58<00:00,  1.12trial/s, best loss: -0.7497104143095739]\n",
      "Re-fitting the model with the best hyper-parameter values found: {'depth': 7.0, 'learning_rate': 0.08576027354875101}\n",
      "Training: Log loss=0.24378055936851875   ROC AUC=0.9104915134370581\n",
      "Validation: Log loss=0.4302933931529751   ROC AUC=0.7497104143095739\n",
      "\n",
      "Performing Bayesian search for hyper-parameters optimization, without replacement and with weights\n",
      "Computed weights\n",
      "For 1010 positive samples: 2.717821782178218\n",
      "For 4480 negative samples: 0.6127232142857143\n",
      "100%|██████████| 200/200 [02:05<00:00,  1.59trial/s, best loss: -0.5805031383007382]\n",
      "Re-fitting the model with the best hyper-parameter values found: {'depth': 5.0, 'learning_rate': 0.06194228315839499}\n",
      "Training: Log loss=0.000346738062438346   ROC AUC=0.6021666150990099\n",
      "Validation: Log loss=0.842022701045374   ROC AUC=0.5805031383007382\n",
      "\n",
      "Performing grid-search for hyper-parameters optimization while maintaining missing data\n",
      "\n",
      "bestTest = 0.7214182044\n",
      "bestIteration = 198\n",
      "\n",
      "0:\tloss: 0.7214182\tbest: 0.7214182 (0)\ttotal: 434ms\tremaining: 57.3s\n",
      "\n",
      "bestTest = 0.7307692308\n",
      "bestIteration = 199\n",
      "\n",
      "1:\tloss: 0.7307692\tbest: 0.7307692 (1)\ttotal: 876ms\tremaining: 57.3s\n",
      "\n",
      "bestTest = 0.7345981063\n",
      "bestIteration = 199\n",
      "\n",
      "2:\tloss: 0.7345981\tbest: 0.7345981 (2)\ttotal: 1.31s\tremaining: 56.6s\n",
      "\n",
      "bestTest = 0.73982307\n",
      "bestIteration = 197\n",
      "\n",
      "3:\tloss: 0.7398231\tbest: 0.7398231 (3)\ttotal: 1.75s\tremaining: 56.4s\n",
      "\n",
      "bestTest = 0.7392217845\n",
      "bestIteration = 199\n",
      "\n",
      "4:\tloss: 0.7392218\tbest: 0.7398231 (3)\ttotal: 2.19s\tremaining: 56.2s\n",
      "\n",
      "bestTest = 0.7413055498\n",
      "bestIteration = 195\n",
      "\n",
      "5:\tloss: 0.7413055\tbest: 0.7413055 (5)\ttotal: 2.65s\tremaining: 56.1s\n",
      "\n",
      "bestTest = 0.742300781\n",
      "bestIteration = 198\n",
      "\n",
      "6:\tloss: 0.7423008\tbest: 0.7423008 (6)\ttotal: 3.1s\tremaining: 55.8s\n",
      "\n",
      "bestTest = 0.7437694381\n",
      "bestIteration = 163\n",
      "\n",
      "7:\tloss: 0.7437694\tbest: 0.7437694 (7)\ttotal: 3.55s\tremaining: 55.5s\n",
      "\n",
      "bestTest = 0.7424424632\n",
      "bestIteration = 195\n",
      "\n",
      "8:\tloss: 0.7424425\tbest: 0.7437694 (7)\ttotal: 4.01s\tremaining: 55.3s\n",
      "\n",
      "bestTest = 0.7436623125\n",
      "bestIteration = 195\n",
      "\n",
      "9:\tloss: 0.7436623\tbest: 0.7437694 (7)\ttotal: 4.46s\tremaining: 54.9s\n",
      "\n",
      "bestTest = 0.746720575\n",
      "bestIteration = 199\n",
      "\n",
      "10:\tloss: 0.7467206\tbest: 0.7467206 (10)\ttotal: 4.92s\tremaining: 54.6s\n",
      "\n",
      "bestTest = 0.7483620153\n",
      "bestIteration = 169\n",
      "\n",
      "11:\tloss: 0.7483620\tbest: 0.7483620 (11)\ttotal: 5.38s\tremaining: 54.2s\n",
      "\n",
      "bestTest = 0.7475464787\n",
      "bestIteration = 137\n",
      "\n",
      "12:\tloss: 0.7475465\tbest: 0.7483620 (11)\ttotal: 5.84s\tremaining: 53.9s\n",
      "\n",
      "bestTest = 0.7479196904\n",
      "bestIteration = 196\n",
      "\n",
      "13:\tloss: 0.7479197\tbest: 0.7483620 (11)\ttotal: 6.3s\tremaining: 53.5s\n",
      "\n",
      "bestTest = 0.7406005944\n",
      "bestIteration = 196\n",
      "\n",
      "14:\tloss: 0.7406006\tbest: 0.7483620 (11)\ttotal: 6.77s\tremaining: 53.2s\n",
      "\n",
      "bestTest = 0.74755339\n",
      "bestIteration = 191\n",
      "\n",
      "15:\tloss: 0.7475534\tbest: 0.7483620 (11)\ttotal: 7.22s\tremaining: 52.8s\n",
      "\n",
      "bestTest = 0.7490773378\n",
      "bestIteration = 197\n",
      "\n",
      "16:\tloss: 0.7490773\tbest: 0.7490773 (16)\ttotal: 7.69s\tremaining: 52.5s\n",
      "\n",
      "bestTest = 0.7498168498\n",
      "bestIteration = 159\n",
      "\n",
      "17:\tloss: 0.7498168\tbest: 0.7498168 (17)\ttotal: 8.15s\tremaining: 52s\n",
      "\n",
      "bestTest = 0.7485797222\n",
      "bestIteration = 149\n",
      "\n",
      "18:\tloss: 0.7485797\tbest: 0.7498168 (17)\ttotal: 8.6s\tremaining: 51.6s\n",
      "\n",
      "bestTest = 0.7287511231\n",
      "bestIteration = 199\n",
      "\n",
      "19:\tloss: 0.7287511\tbest: 0.7498168 (17)\ttotal: 9.12s\tremaining: 51.5s\n",
      "\n",
      "bestTest = 0.7340901237\n",
      "bestIteration = 199\n",
      "\n",
      "20:\tloss: 0.7340901\tbest: 0.7498168 (17)\ttotal: 9.63s\tremaining: 51.4s\n",
      "\n",
      "bestTest = 0.7380641371\n",
      "bestIteration = 197\n",
      "\n",
      "21:\tloss: 0.7380641\tbest: 0.7498168 (17)\ttotal: 10.1s\tremaining: 51.2s\n",
      "\n",
      "bestTest = 0.7414610547\n",
      "bestIteration = 197\n",
      "\n",
      "22:\tloss: 0.7414611\tbest: 0.7498168 (17)\ttotal: 10.7s\tremaining: 51s\n",
      "\n",
      "bestTest = 0.7431992536\n",
      "bestIteration = 198\n",
      "\n",
      "23:\tloss: 0.7431993\tbest: 0.7498168 (17)\ttotal: 11.2s\tremaining: 50.7s\n",
      "\n",
      "bestTest = 0.7443188887\n",
      "bestIteration = 198\n",
      "\n",
      "24:\tloss: 0.7443189\tbest: 0.7498168 (17)\ttotal: 11.7s\tremaining: 50.5s\n",
      "\n",
      "bestTest = 0.7451862603\n",
      "bestIteration = 199\n",
      "\n",
      "25:\tloss: 0.7451863\tbest: 0.7498168 (17)\ttotal: 12.2s\tremaining: 50.3s\n",
      "\n",
      "bestTest = 0.7429953694\n",
      "bestIteration = 192\n",
      "\n",
      "26:\tloss: 0.7429954\tbest: 0.7498168 (17)\ttotal: 12.7s\tremaining: 50.1s\n",
      "\n",
      "bestTest = 0.7470834197\n",
      "bestIteration = 179\n",
      "\n",
      "27:\tloss: 0.7470834\tbest: 0.7498168 (17)\ttotal: 13.3s\tremaining: 49.7s\n",
      "\n",
      "bestTest = 0.7477710968\n",
      "bestIteration = 183\n",
      "\n",
      "28:\tloss: 0.7477711\tbest: 0.7498168 (17)\ttotal: 13.8s\tremaining: 49.5s\n",
      "\n",
      "bestTest = 0.7433651254\n",
      "bestIteration = 88\n",
      "\n",
      "29:\tloss: 0.7433651\tbest: 0.7498168 (17)\ttotal: 14.3s\tremaining: 49.2s\n",
      "\n",
      "bestTest = 0.7471387103\n",
      "bestIteration = 98\n",
      "\n",
      "30:\tloss: 0.7471387\tbest: 0.7498168 (17)\ttotal: 14.8s\tremaining: 48.9s\n",
      "\n",
      "bestTest = 0.7504354136\n",
      "bestIteration = 121\n",
      "\n",
      "31:\tloss: 0.7504354\tbest: 0.7504354 (31)\ttotal: 15.4s\tremaining: 48.5s\n",
      "\n",
      "bestTest = 0.7488630866\n",
      "bestIteration = 156\n",
      "\n",
      "32:\tloss: 0.7488631\tbest: 0.7504354 (31)\ttotal: 15.9s\tremaining: 48.2s\n",
      "\n",
      "bestTest = 0.7504043127\n",
      "bestIteration = 127\n",
      "\n",
      "33:\tloss: 0.7504043\tbest: 0.7504354 (31)\ttotal: 16.4s\tremaining: 47.8s\n",
      "\n",
      "bestTest = 0.7472423803\n",
      "bestIteration = 128\n",
      "\n",
      "34:\tloss: 0.7472424\tbest: 0.7504354 (31)\ttotal: 17s\tremaining: 47.5s\n",
      "\n",
      "bestTest = 0.7448579722\n",
      "bestIteration = 71\n",
      "\n",
      "35:\tloss: 0.7448580\tbest: 0.7504354 (31)\ttotal: 17.5s\tremaining: 47.1s\n",
      "\n",
      "bestTest = 0.7399958532\n",
      "bestIteration = 158\n",
      "\n",
      "36:\tloss: 0.7399959\tbest: 0.7504354 (31)\ttotal: 18s\tremaining: 46.7s\n",
      "\n",
      "bestTest = 0.7414817887\n",
      "bestIteration = 84\n",
      "\n",
      "37:\tloss: 0.7414818\tbest: 0.7504354 (31)\ttotal: 18.5s\tremaining: 46.3s\n",
      "\n",
      "bestTest = 0.7324866957\n",
      "bestIteration = 198\n",
      "\n",
      "38:\tloss: 0.7324867\tbest: 0.7504354 (31)\ttotal: 19.1s\tremaining: 46.1s\n",
      "\n",
      "bestTest = 0.7384477158\n",
      "bestIteration = 173\n",
      "\n",
      "39:\tloss: 0.7384477\tbest: 0.7504354 (31)\ttotal: 19.7s\tremaining: 45.8s\n",
      "\n",
      "bestTest = 0.7411120326\n",
      "bestIteration = 199\n",
      "\n",
      "40:\tloss: 0.7411120\tbest: 0.7504354 (31)\ttotal: 20.3s\tremaining: 45.5s\n",
      "\n",
      "bestTest = 0.7415958256\n",
      "bestIteration = 191\n",
      "\n",
      "41:\tloss: 0.7415958\tbest: 0.7504354 (31)\ttotal: 20.9s\tremaining: 45.2s\n",
      "\n",
      "bestTest = 0.7445953418\n",
      "bestIteration = 191\n",
      "\n",
      "42:\tloss: 0.7445953\tbest: 0.7504354 (31)\ttotal: 21.5s\tremaining: 44.9s\n",
      "\n",
      "bestTest = 0.749187919\n",
      "bestIteration = 167\n",
      "\n",
      "43:\tloss: 0.7491879\tbest: 0.7504354 (31)\ttotal: 22.1s\tremaining: 44.6s\n",
      "\n",
      "bestTest = 0.7508915613\n",
      "bestIteration = 175\n",
      "\n",
      "44:\tloss: 0.7508916\tbest: 0.7508916 (44)\ttotal: 22.6s\tremaining: 44.3s\n",
      "\n",
      "bestTest = 0.7473771512\n",
      "bestIteration = 154\n",
      "\n",
      "45:\tloss: 0.7473772\tbest: 0.7508916 (44)\ttotal: 23.2s\tremaining: 43.9s\n",
      "\n",
      "bestTest = 0.7484656853\n",
      "bestIteration = 179\n",
      "\n",
      "46:\tloss: 0.7484657\tbest: 0.7508916 (44)\ttotal: 23.8s\tremaining: 43.6s\n",
      "\n",
      "bestTest = 0.7468553459\n",
      "bestIteration = 154\n",
      "\n",
      "47:\tloss: 0.7468553\tbest: 0.7508916 (44)\ttotal: 24.4s\tremaining: 43.2s\n",
      "\n",
      "bestTest = 0.7487006704\n",
      "bestIteration = 113\n",
      "\n",
      "48:\tloss: 0.7487007\tbest: 0.7508916 (44)\ttotal: 25s\tremaining: 42.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bestTest = 0.7439767779\n",
      "bestIteration = 136\n",
      "\n",
      "49:\tloss: 0.7439768\tbest: 0.7508916 (44)\ttotal: 25.6s\tremaining: 42.5s\n",
      "\n",
      "bestTest = 0.7464579446\n",
      "bestIteration = 94\n",
      "\n",
      "50:\tloss: 0.7464579\tbest: 0.7508916 (44)\ttotal: 26.2s\tremaining: 42.1s\n",
      "\n",
      "bestTest = 0.7466030825\n",
      "bestIteration = 152\n",
      "\n",
      "51:\tloss: 0.7466031\tbest: 0.7508916 (44)\ttotal: 26.8s\tremaining: 41.7s\n",
      "\n",
      "bestTest = 0.7465063239\n",
      "bestIteration = 94\n",
      "\n",
      "52:\tloss: 0.7465063\tbest: 0.7508916 (44)\ttotal: 27.4s\tremaining: 41.3s\n",
      "\n",
      "bestTest = 0.7459361393\n",
      "bestIteration = 102\n",
      "\n",
      "53:\tloss: 0.7459361\tbest: 0.7508916 (44)\ttotal: 27.9s\tremaining: 40.9s\n",
      "\n",
      "bestTest = 0.7398057917\n",
      "bestIteration = 77\n",
      "\n",
      "54:\tloss: 0.7398058\tbest: 0.7508916 (44)\ttotal: 28.5s\tremaining: 40.5s\n",
      "\n",
      "bestTest = 0.7413055498\n",
      "bestIteration = 82\n",
      "\n",
      "55:\tloss: 0.7413055\tbest: 0.7508916 (44)\ttotal: 29.1s\tremaining: 40s\n",
      "\n",
      "bestTest = 0.7497822932\n",
      "bestIteration = 59\n",
      "\n",
      "56:\tloss: 0.7497823\tbest: 0.7508916 (44)\ttotal: 29.7s\tremaining: 39.6s\n",
      "\n",
      "bestTest = 0.7323242795\n",
      "bestIteration = 199\n",
      "\n",
      "57:\tloss: 0.7323243\tbest: 0.7508916 (44)\ttotal: 30.4s\tremaining: 39.3s\n",
      "\n",
      "bestTest = 0.738178174\n",
      "bestIteration = 196\n",
      "\n",
      "58:\tloss: 0.7381782\tbest: 0.7508916 (44)\ttotal: 31s\tremaining: 38.9s\n",
      "\n",
      "bestTest = 0.7416753058\n",
      "bestIteration = 199\n",
      "\n",
      "59:\tloss: 0.7416753\tbest: 0.7508916 (44)\ttotal: 31.7s\tremaining: 38.5s\n",
      "\n",
      "bestTest = 0.7442255857\n",
      "bestIteration = 199\n",
      "\n",
      "60:\tloss: 0.7442256\tbest: 0.7508916 (44)\ttotal: 32.3s\tremaining: 38.1s\n",
      "\n",
      "bestTest = 0.7426947267\n",
      "bestIteration = 199\n",
      "\n",
      "61:\tloss: 0.7426947\tbest: 0.7508916 (44)\ttotal: 32.9s\tremaining: 37.7s\n",
      "\n",
      "bestTest = 0.741405764\n",
      "bestIteration = 194\n",
      "\n",
      "62:\tloss: 0.7414058\tbest: 0.7508916 (44)\ttotal: 33.6s\tremaining: 37.3s\n",
      "\n",
      "bestTest = 0.7449996544\n",
      "bestIteration = 188\n",
      "\n",
      "63:\tloss: 0.7449997\tbest: 0.7508916 (44)\ttotal: 34.2s\tremaining: 36.9s\n",
      "\n",
      "bestTest = 0.7466825627\n",
      "bestIteration = 162\n",
      "\n",
      "64:\tloss: 0.7466826\tbest: 0.7508916 (44)\ttotal: 34.9s\tremaining: 36.5s\n",
      "\n",
      "bestTest = 0.7426325247\n",
      "bestIteration = 80\n",
      "\n",
      "65:\tloss: 0.7426325\tbest: 0.7508916 (44)\ttotal: 35.6s\tremaining: 36.1s\n",
      "\n",
      "bestTest = 0.7480959292\n",
      "bestIteration = 106\n",
      "\n",
      "66:\tloss: 0.7480959\tbest: 0.7508916 (44)\ttotal: 36.2s\tremaining: 35.7s\n",
      "\n",
      "bestTest = 0.742670537\n",
      "bestIteration = 160\n",
      "\n",
      "67:\tloss: 0.7426705\tbest: 0.7508916 (44)\ttotal: 36.9s\tremaining: 35.2s\n",
      "\n",
      "bestTest = 0.7462782501\n",
      "bestIteration = 98\n",
      "\n",
      "68:\tloss: 0.7462783\tbest: 0.7508916 (44)\ttotal: 37.5s\tremaining: 34.8s\n",
      "\n",
      "bestTest = 0.745117147\n",
      "bestIteration = 176\n",
      "\n",
      "69:\tloss: 0.7451171\tbest: 0.7508916 (44)\ttotal: 38.2s\tremaining: 34.4s\n",
      "\n",
      "bestTest = 0.7403517866\n",
      "bestIteration = 44\n",
      "\n",
      "70:\tloss: 0.7403518\tbest: 0.7508916 (44)\ttotal: 38.8s\tremaining: 33.9s\n",
      "\n",
      "bestTest = 0.7417513304\n",
      "bestIteration = 85\n",
      "\n",
      "71:\tloss: 0.7417513\tbest: 0.7508916 (44)\ttotal: 39.5s\tremaining: 33.5s\n",
      "\n",
      "bestTest = 0.7412986385\n",
      "bestIteration = 80\n",
      "\n",
      "72:\tloss: 0.7412986\tbest: 0.7508916 (44)\ttotal: 40.1s\tremaining: 33s\n",
      "\n",
      "bestTest = 0.7397159444\n",
      "bestIteration = 42\n",
      "\n",
      "73:\tloss: 0.7397159\tbest: 0.7508916 (44)\ttotal: 40.8s\tremaining: 32.5s\n",
      "\n",
      "bestTest = 0.7448476052\n",
      "bestIteration = 45\n",
      "\n",
      "74:\tloss: 0.7448476\tbest: 0.7508916 (44)\ttotal: 41.5s\tremaining: 32.1s\n",
      "\n",
      "bestTest = 0.7402930403\n",
      "bestIteration = 44\n",
      "\n",
      "75:\tloss: 0.7402930\tbest: 0.7508916 (44)\ttotal: 42.1s\tremaining: 31.6s\n",
      "\n",
      "bestTest = 0.7354274656\n",
      "bestIteration = 194\n",
      "\n",
      "76:\tloss: 0.7354275\tbest: 0.7508916 (44)\ttotal: 42.8s\tremaining: 31.2s\n",
      "\n",
      "bestTest = 0.7407353653\n",
      "bestIteration = 198\n",
      "\n",
      "77:\tloss: 0.7407354\tbest: 0.7508916 (44)\ttotal: 43.6s\tremaining: 30.7s\n",
      "\n",
      "bestTest = 0.7408321239\n",
      "bestIteration = 181\n",
      "\n",
      "78:\tloss: 0.7408321\tbest: 0.7508916 (44)\ttotal: 44.3s\tremaining: 30.3s\n",
      "\n",
      "bestTest = 0.7409461608\n",
      "bestIteration = 186\n",
      "\n",
      "79:\tloss: 0.7409462\tbest: 0.7508916 (44)\ttotal: 45.1s\tremaining: 29.9s\n",
      "\n",
      "bestTest = 0.7431301403\n",
      "bestIteration = 195\n",
      "\n",
      "80:\tloss: 0.7431301\tbest: 0.7508916 (44)\ttotal: 45.8s\tremaining: 29.4s\n",
      "\n",
      "bestTest = 0.7400718778\n",
      "bestIteration = 94\n",
      "\n",
      "81:\tloss: 0.7400719\tbest: 0.7508916 (44)\ttotal: 46.6s\tremaining: 29s\n",
      "\n",
      "bestTest = 0.7417755201\n",
      "bestIteration = 188\n",
      "\n",
      "82:\tloss: 0.7417755\tbest: 0.7508916 (44)\ttotal: 47.3s\tremaining: 28.5s\n",
      "\n",
      "bestTest = 0.7425392218\n",
      "bestIteration = 108\n",
      "\n",
      "83:\tloss: 0.7425392\tbest: 0.7508916 (44)\ttotal: 48s\tremaining: 28s\n",
      "\n",
      "bestTest = 0.7375285092\n",
      "bestIteration = 151\n",
      "\n",
      "84:\tloss: 0.7375285\tbest: 0.7508916 (44)\ttotal: 48.8s\tremaining: 27.6s\n",
      "\n",
      "bestTest = 0.7442324971\n",
      "bestIteration = 149\n",
      "\n",
      "85:\tloss: 0.7442325\tbest: 0.7508916 (44)\ttotal: 49.5s\tremaining: 27.1s\n",
      "\n",
      "bestTest = 0.7387621812\n",
      "bestIteration = 74\n",
      "\n",
      "86:\tloss: 0.7387622\tbest: 0.7508916 (44)\ttotal: 50.3s\tremaining: 26.6s\n",
      "\n",
      "bestTest = 0.7461192895\n",
      "bestIteration = 71\n",
      "\n",
      "87:\tloss: 0.7461193\tbest: 0.7508916 (44)\ttotal: 51s\tremaining: 26.1s\n",
      "\n",
      "bestTest = 0.7371241966\n",
      "bestIteration = 135\n",
      "\n",
      "88:\tloss: 0.7371242\tbest: 0.7508916 (44)\ttotal: 51.8s\tremaining: 25.6s\n",
      "\n",
      "bestTest = 0.7406282397\n",
      "bestIteration = 31\n",
      "\n",
      "89:\tloss: 0.7406282\tbest: 0.7508916 (44)\ttotal: 52.5s\tremaining: 25.1s\n",
      "\n",
      "bestTest = 0.7384477158\n",
      "bestIteration = 77\n",
      "\n",
      "90:\tloss: 0.7384477\tbest: 0.7508916 (44)\ttotal: 53.3s\tremaining: 24.6s\n",
      "\n",
      "bestTest = 0.7435586426\n",
      "bestIteration = 65\n",
      "\n",
      "91:\tloss: 0.7435586\tbest: 0.7508916 (44)\ttotal: 54s\tremaining: 24.1s\n",
      "\n",
      "bestTest = 0.7407353653\n",
      "bestIteration = 24\n",
      "\n",
      "92:\tloss: 0.7407354\tbest: 0.7508916 (44)\ttotal: 54.8s\tremaining: 23.6s\n",
      "\n",
      "bestTest = 0.7434998963\n",
      "bestIteration = 38\n",
      "\n",
      "93:\tloss: 0.7434999\tbest: 0.7508916 (44)\ttotal: 55.5s\tremaining: 23s\n",
      "\n",
      "bestTest = 0.7436450342\n",
      "bestIteration = 40\n",
      "\n",
      "94:\tloss: 0.7436450\tbest: 0.7508916 (44)\ttotal: 56.3s\tremaining: 22.5s\n",
      "\n",
      "bestTest = 0.7369479577\n",
      "bestIteration = 194\n",
      "\n",
      "95:\tloss: 0.7369480\tbest: 0.7508916 (44)\ttotal: 57.1s\tremaining: 22s\n",
      "\n",
      "bestTest = 0.736616214\n",
      "bestIteration = 187\n",
      "\n",
      "96:\tloss: 0.7366162\tbest: 0.7508916 (44)\ttotal: 58s\tremaining: 21.5s\n",
      "\n",
      "bestTest = 0.7427603843\n",
      "bestIteration = 199\n",
      "\n",
      "97:\tloss: 0.7427604\tbest: 0.7508916 (44)\ttotal: 58.9s\tremaining: 21s\n",
      "\n",
      "bestTest = 0.7417893427\n",
      "bestIteration = 196\n",
      "\n",
      "98:\tloss: 0.7417893\tbest: 0.7508916 (44)\ttotal: 59.8s\tremaining: 20.5s\n",
      "\n",
      "bestTest = 0.7493399682\n",
      "bestIteration = 190\n",
      "\n",
      "99:\tloss: 0.7493400\tbest: 0.7508916 (44)\ttotal: 1m\tremaining: 20s\n",
      "\n",
      "bestTest = 0.7452588292\n",
      "bestIteration = 164\n",
      "\n",
      "100:\tloss: 0.7452588\tbest: 0.7508916 (44)\ttotal: 1m 1s\tremaining: 19.5s\n",
      "\n",
      "bestTest = 0.7428605985\n",
      "bestIteration = 113\n",
      "\n",
      "101:\tloss: 0.7428606\tbest: 0.7508916 (44)\ttotal: 1m 2s\tremaining: 19s\n",
      "\n",
      "bestTest = 0.7358559679\n",
      "bestIteration = 56\n",
      "\n",
      "102:\tloss: 0.7358560\tbest: 0.7508916 (44)\ttotal: 1m 3s\tremaining: 18.5s\n",
      "\n",
      "bestTest = 0.746720575\n",
      "bestIteration = 69\n",
      "\n",
      "103:\tloss: 0.7467206\tbest: 0.7508916 (44)\ttotal: 1m 4s\tremaining: 17.9s\n",
      "\n",
      "bestTest = 0.7444502039\n",
      "bestIteration = 29\n",
      "\n",
      "104:\tloss: 0.7444502\tbest: 0.7508916 (44)\ttotal: 1m 5s\tremaining: 17.4s\n",
      "\n",
      "bestTest = 0.7462506047\n",
      "bestIteration = 59\n",
      "\n",
      "105:\tloss: 0.7462506\tbest: 0.7508916 (44)\ttotal: 1m 6s\tremaining: 16.9s\n",
      "\n",
      "bestTest = 0.7413193725\n",
      "bestIteration = 81\n",
      "\n",
      "106:\tloss: 0.7413194\tbest: 0.7508916 (44)\ttotal: 1m 7s\tremaining: 16.3s\n",
      "\n",
      "bestTest = 0.736225724\n",
      "bestIteration = 50\n",
      "\n",
      "107:\tloss: 0.7362257\tbest: 0.7508916 (44)\ttotal: 1m 8s\tremaining: 15.7s\n",
      "\n",
      "bestTest = 0.7432649112\n",
      "bestIteration = 41\n",
      "\n",
      "108:\tloss: 0.7432649\tbest: 0.7508916 (44)\ttotal: 1m 8s\tremaining: 15.2s\n",
      "\n",
      "bestTest = 0.7397470454\n",
      "bestIteration = 33\n",
      "\n",
      "109:\tloss: 0.7397470\tbest: 0.7508916 (44)\ttotal: 1m 9s\tremaining: 14.6s\n",
      "\n",
      "bestTest = 0.7391181146\n",
      "bestIteration = 36\n",
      "\n",
      "110:\tloss: 0.7391181\tbest: 0.7508916 (44)\ttotal: 1m 10s\tremaining: 14s\n",
      "\n",
      "bestTest = 0.7385410187\n",
      "bestIteration = 13\n",
      "\n",
      "111:\tloss: 0.7385410\tbest: 0.7508916 (44)\ttotal: 1m 11s\tremaining: 13.4s\n",
      "\n",
      "bestTest = 0.7440078789\n",
      "bestIteration = 34\n",
      "\n",
      "112:\tloss: 0.7440079\tbest: 0.7508916 (44)\ttotal: 1m 12s\tremaining: 12.8s\n",
      "\n",
      "bestTest = 0.739463681\n",
      "bestIteration = 35\n",
      "\n",
      "113:\tloss: 0.7394637\tbest: 0.7508916 (44)\ttotal: 1m 13s\tremaining: 12.2s\n",
      "\n",
      "bestTest = 0.7360840417\n",
      "bestIteration = 184\n",
      "\n",
      "114:\tloss: 0.7360840\tbest: 0.7508916 (44)\ttotal: 1m 14s\tremaining: 11.7s\n",
      "\n",
      "bestTest = 0.7383094893\n",
      "bestIteration = 190\n",
      "\n",
      "115:\tloss: 0.7383095\tbest: 0.7508916 (44)\ttotal: 1m 16s\tremaining: 11.1s\n",
      "\n",
      "bestTest = 0.7419344806\n",
      "bestIteration = 196\n",
      "\n",
      "116:\tloss: 0.7419345\tbest: 0.7508916 (44)\ttotal: 1m 17s\tremaining: 10.6s\n",
      "\n",
      "bestTest = 0.7324452277\n",
      "bestIteration = 140\n",
      "\n",
      "117:\tloss: 0.7324452\tbest: 0.7508916 (44)\ttotal: 1m 18s\tremaining: 10s\n",
      "\n",
      "bestTest = 0.7388865851\n",
      "bestIteration = 83\n",
      "\n",
      "118:\tloss: 0.7388866\tbest: 0.7508916 (44)\ttotal: 1m 20s\tremaining: 9.44s\n",
      "\n",
      "bestTest = 0.7375630659\n",
      "bestIteration = 99\n",
      "\n",
      "119:\tloss: 0.7375631\tbest: 0.7508916 (44)\ttotal: 1m 21s\tremaining: 8.86s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bestTest = 0.735372175\n",
      "bestIteration = 100\n",
      "\n",
      "120:\tloss: 0.7353722\tbest: 0.7508916 (44)\ttotal: 1m 23s\tremaining: 8.23s\n",
      "\n",
      "bestTest = 0.739073191\n",
      "bestIteration = 62\n",
      "\n",
      "121:\tloss: 0.7390732\tbest: 0.7508916 (44)\ttotal: 1m 24s\tremaining: 7.6s\n",
      "\n",
      "bestTest = 0.7320962057\n",
      "bestIteration = 54\n",
      "\n",
      "122:\tloss: 0.7320962\tbest: 0.7508916 (44)\ttotal: 1m 25s\tremaining: 6.96s\n",
      "\n",
      "bestTest = 0.7377012924\n",
      "bestIteration = 70\n",
      "\n",
      "123:\tloss: 0.7377013\tbest: 0.7508916 (44)\ttotal: 1m 26s\tremaining: 6.3s\n",
      "\n",
      "bestTest = 0.7359838275\n",
      "bestIteration = 55\n",
      "\n",
      "124:\tloss: 0.7359838\tbest: 0.7508916 (44)\ttotal: 1m 28s\tremaining: 5.64s\n",
      "\n",
      "bestTest = 0.733696178\n",
      "bestIteration = 28\n",
      "\n",
      "125:\tloss: 0.7336962\tbest: 0.7508916 (44)\ttotal: 1m 29s\tremaining: 4.96s\n",
      "\n",
      "bestTest = 0.735721197\n",
      "bestIteration = 24\n",
      "\n",
      "126:\tloss: 0.7357212\tbest: 0.7508916 (44)\ttotal: 1m 30s\tremaining: 4.29s\n",
      "\n",
      "bestTest = 0.7339207962\n",
      "bestIteration = 10\n",
      "\n",
      "127:\tloss: 0.7339208\tbest: 0.7508916 (44)\ttotal: 1m 32s\tremaining: 3.61s\n",
      "\n",
      "bestTest = 0.7359043472\n",
      "bestIteration = 25\n",
      "\n",
      "128:\tloss: 0.7359043\tbest: 0.7508916 (44)\ttotal: 1m 33s\tremaining: 2.9s\n",
      "\n",
      "bestTest = 0.7348330914\n",
      "bestIteration = 59\n",
      "\n",
      "129:\tloss: 0.7348331\tbest: 0.7508916 (44)\ttotal: 1m 35s\tremaining: 2.2s\n",
      "\n",
      "bestTest = 0.7309178243\n",
      "bestIteration = 23\n",
      "\n",
      "130:\tloss: 0.7309178\tbest: 0.7508916 (44)\ttotal: 1m 36s\tremaining: 1.47s\n",
      "\n",
      "bestTest = 0.735382542\n",
      "bestIteration = 29\n",
      "\n",
      "131:\tloss: 0.7353825\tbest: 0.7508916 (44)\ttotal: 1m 37s\tremaining: 741ms\n",
      "\n",
      "bestTest = 0.7281584767\n",
      "bestIteration = 10\n",
      "\n",
      "132:\tloss: 0.7281585\tbest: 0.7508916 (44)\ttotal: 1m 39s\tremaining: 0us\n",
      "Estimating final quality...\n",
      "Best params {'depth': 4, 'learning_rate': 0.06999999999999999} with AUC 0.7591541425456522 obtained at iteration 147 with Log loss 0.41110856897360565\n",
      "\n",
      "Cross-validating model without imputation.\n",
      "Best validation with parameters {'iterations': 200, 'learning_rate': 0.08576027354875101, 'depth': 7.0, 'eval_metric': 'AUC:hints=skip_train~false', 'random_state': 42, 'loss_function': 'Logloss'} achieved at iteration 41\n",
      "Training: Logloss 0.36435089760508077   ROC AUC 0.8362407666828217\n",
      "Validation: Logloss 0.41374882288660236   ROC AUC 0.7528561244874519\n",
      "Re-fitting the model and testing it\n",
      "Iteration: 41\n",
      "Training (on dev. set): Log loss=0.3932154661728572   ROC AUC=0.7956916723980725\n",
      "Test (on test set): Log loss=0.424345152026373   ROC AUC=0.7803172861904561\n",
      "\n",
      "Cross-validating model with imputation.\n",
      "Best validation with parameters {'iterations': 200, 'learning_rate': 0.05434741145879128, 'depth': 6.0, 'eval_metric': 'AUC:hints=skip_train~false', 'random_state': 42, 'loss_function': 'Logloss'} achieved at iteration 168\n",
      "Training: Logloss 0.33322238483057903   ROC AUC 0.8738270666803276\n",
      "Validation: Logloss 0.41295525414218837   ROC AUC 0.7554402226665304\n",
      "Re-fitting the model and testing it\n",
      "Iteration: 168\n",
      "Training (on dev. set): Log loss=0.373443652633036   ROC AUC=0.8152075291629609\n",
      "Test (on test set): Log loss=0.4115702857087924   ROC AUC=0.7870132061938315\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' TODO\\nTry Karpathy approach\\nLeverage Tensorboard\\nHow to display CatBoost charts outside of notebook? Is it possible?\\nExplore Seaborne\\nUse the whole HANES dataset from CDC, and also try with GPU\\nTry other strategies for imputation based on mean encoding and similar\\nInstead of checking if survival after 10 years, estimate the number of years of survival\\nC-index is the same as the ROC AUC for logistic regression.\\n   see https://www.statisticshowto.com/c-statistic/#:~:text=A%20weighted%20c-index%20is,correctly%20predicting%20a%20negative%20outcome\\n   and also https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4886856/  and https://bit.ly/3dvUh07\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shap\n",
    "import sklearn\n",
    "import pydotplus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, space_eval, hp\n",
    "\n",
    "from util import load_data\n",
    "\n",
    "\n",
    "def print_train_val_test_c_indices(classifier,\n",
    "                                   X_train,\n",
    "                                   y_train,\n",
    "                                   X_val,\n",
    "                                   y_val,\n",
    "                                   X_test,\n",
    "                                   y_test):\n",
    "    y_train_preds = classifier.predict_proba(X_train)[:, 1]\n",
    "    print(f'Train ROC AUC: {roc_auc_score(y_train, y_train_preds)}')\n",
    "\n",
    "    y_val_preds = classifier.predict_proba(X_val)[:, 1]\n",
    "    print(f'Val ROC AUC: {roc_auc_score(y_val, y_val_preds)}')\n",
    "\n",
    "    y_test_preds = classifier.predict_proba(X_test)[:, 1]\n",
    "    print(f'Test ROC AUC: {roc_auc_score(y_test, y_test_preds)}')\n",
    "\n",
    "\n",
    "def make_imputed_pool(X, y, imputer, cat_features, weight=None):\n",
    "    X_imputed = X if imputer is None else pd.DataFrame(imputer.transform(X), columns=X.columns)\n",
    "    # imputer.transform() above has converted the int columns with categories into float, need to be converted back to int\n",
    "    X_imputed = X_imputed.astype({'Sex': int, 'Race': int})\n",
    "    pool = Pool(data=X_imputed, label=y, cat_features=cat_features, weight=weight)\n",
    "    return pool, X_imputed\n",
    "\n",
    "\n",
    "seed = 42\n",
    "iterations = 200\n",
    "hyper_iterations = 200\n",
    "cv_folds = 4\n",
    "\n",
    "# Load the NHANES I epidemiology dataset\n",
    "X_dev, X_test, y_dev, y_test = load_data(10)\n",
    "\n",
    "# Convert categorical features from float to int, as that is what CatBoost expects\n",
    "X_dev = X_dev.astype({'Sex': int, 'Race': int})\n",
    "y_dev = y_dev.astype(int)\n",
    "X_test = X_test.astype({'Sex': int, 'Race': int})\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "\n",
    "# Find out how many samples have missing data in one or more variables (columns)\n",
    "def count_samples_with_missing_data(df):\n",
    "    res = sum(df.isnull().any(axis='columns'))\n",
    "    return res\n",
    "\n",
    "\n",
    "dev_missing_count = count_samples_with_missing_data(X_dev)\n",
    "test_missing_count = count_samples_with_missing_data(X_test)\n",
    "\n",
    "print('Dev. set missing data in', dev_missing_count, 'samples out of', len(X_dev))\n",
    "print('Test set missing data in', test_missing_count, 'samples out of', len(X_test))\n",
    "\n",
    "# Split the dev set into training and validation. The latter will be used for hyper-parameters tuning.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Make a dataset after dropping samples with missing data (note, no samples with missing data in test set)\n",
    "X_dev_dropped = X_dev.dropna(axis='rows')\n",
    "y_dev_dropped = y_dev.loc[X_dev_dropped.index]\n",
    "X_train_dropped = X_train.dropna(axis='rows')\n",
    "y_train_dropped = y_train.loc[X_train_dropped.index]\n",
    "X_val_dropped = X_val.dropna(axis='rows')\n",
    "y_val_dropped = y_val.loc[X_val_dropped.index]\n",
    "\n",
    "# Now impute missing values using the mean, instead of dropping samples containing them\n",
    "mean_imputer = SimpleImputer(strategy='mean', verbose=0)\n",
    "mean_imputer.fit(X_dev)  # TODO should this be fit on val or dev dataset?\n",
    "\n",
    "iter_imputer = IterativeImputer(random_state=seed, sample_posterior=False, max_iter=10, min_value=0, verbose=0)\n",
    "iter_imputer.fit(X_dev)  # TODO should this be fit on val or dev dataset?\n",
    "\n",
    "cat_features = [3, 11]  # Categorical features are race and sex\n",
    "\n",
    "\n",
    "def compute_weights(y):\n",
    "    \"\"\"\n",
    "    Computes and returns the weights for every sample, such that if the positive samples are n\n",
    "    times the negative samples, then their weight is 1/n times the weight of the negative samples, and such that\n",
    "    the sum of the weights of all the samples is equal to the total number of samples. One weight value is assigned\n",
    "    to all positive samples, and another to all negative samples.\n",
    "    :param y: an array-like with the ground truth for the samples, with 1 for positive and 0 for negative.\n",
    "    :return: a pair, the first element is a numpy array with the requested weights per sample, the second is a\n",
    "    dictionary providing the count of positive and negative elements, and the assigned respective weights.\n",
    "    \"\"\"\n",
    "    total_pos = sum(y)\n",
    "    total_neg = len(y) - total_pos\n",
    "    # pos_weight = total_neg / total_pos\n",
    "    # neg_weight = total_pos / total_neg\n",
    "    pos_weight = total_neg * len(y) / (2 * total_neg * total_pos)\n",
    "    neg_weight = total_pos * len(y) / (2 * total_neg * total_pos)\n",
    "    assert np.isclose(pos_weight * total_pos + neg_weight * total_neg, len(y))\n",
    "    w = np.full_like(y, neg_weight)\n",
    "    w[y == 1] = pos_weight\n",
    "    return w, {'total_pos': total_pos, 'total_neg': total_neg, 'pos_weight': pos_weight, 'neg_weight': neg_weight}\n",
    "\n",
    "\n",
    "def run_exp_bayes_hyperparams_opt(X_train,\n",
    "                                  y_train,\n",
    "                                  X_val,\n",
    "                                  y_val,\n",
    "                                  cat_features,\n",
    "                                  param_space,\n",
    "                                  max_evals,\n",
    "                                  imputer,\n",
    "                                  weights=None):\n",
    "    train_pool, X_train_imputed = make_imputed_pool(X_train,\n",
    "                                                    y=y_train,\n",
    "                                                    imputer=imputer,\n",
    "                                                    cat_features=cat_features,\n",
    "                                                    weight=weights)\n",
    "\n",
    "    val_pool, X_val_imputed = make_imputed_pool(X_val,\n",
    "                                                y=y_val,\n",
    "                                                imputer=imputer,\n",
    "                                                cat_features=cat_features)\n",
    "\n",
    "    # The objective function, that hyperopt will minimize\n",
    "    def objective(params):\n",
    "        model = CatBoostClassifier(iterations=params['iterations'],\n",
    "                                   eval_metric='AUC:hints=skip_train~false',\n",
    "                                   learning_rate=params['learning_rate'],\n",
    "                                   depth=params['depth'],\n",
    "                                   random_state=params['seed'])\n",
    "        training_res = model.fit(train_pool, eval_set=val_pool, verbose=False)\n",
    "        auc = training_res.best_score_['validation']['AUC']\n",
    "        return -auc  # The objective function is minimized\n",
    "\n",
    "    rstate = np.random.RandomState(seed)\n",
    "    best = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=max_evals, rstate=rstate)\n",
    "    print('Re-fitting the model with the best hyper-parameter values found:', best)\n",
    "    refit_model = CatBoostClassifier(iterations=param_space['iterations'],\n",
    "                                     eval_metric='AUC:hints=skip_train~false',\n",
    "                                     **best,\n",
    "                                     random_state=param_space['seed'])\n",
    "    training_res = refit_model.fit(train_pool, eval_set=val_pool, verbose=False)\n",
    "\n",
    "    y_train_preds = refit_model.predict_proba(X_train)[:, 1]\n",
    "    training_AUC = roc_auc_score(y_train, y_train_preds)\n",
    "    print(f\"Training: Log loss={training_res.best_score_['learn']['Logloss']}   ROC AUC={training_AUC}\")\n",
    "    print(\n",
    "        f\"Validation: Log loss={training_res.best_score_['validation']['Logloss']}   ROC AUC={training_res.best_score_['validation']['AUC']}\")\n",
    "    return refit_model\n",
    "\n",
    "\n",
    "''' Use the iterative imputer, but use Bayesian optimization for the hyper-parameters, instead of grid search. Here\n",
    "we use the train/val data sets\n",
    "\n",
    "Note: passing a CatBoost Pool() instance in the param_space values here below doesn't work, because hyperopt would\n",
    "throw an exception during optimization.'''\n",
    "param_space = {'learning_rate': hp.uniform('learning_rate', .01, .1),\n",
    "               'depth': hp.quniform('depth', 2, 8, 1),\n",
    "               'seed': seed,  # hyperopt accepts constant value parameters\n",
    "               'iterations': iterations\n",
    "               }\n",
    "\n",
    "print('\\nPerforming Bayesian search for hyper-parameters optimization, after dropping samples with missing data')\n",
    "\n",
    "run_exp_bayes_hyperparams_opt(X_train_dropped,\n",
    "                              y_train_dropped,\n",
    "                              X_val_dropped,\n",
    "                              y_val_dropped,\n",
    "                              cat_features=cat_features,\n",
    "                              param_space=param_space,\n",
    "                              max_evals=hyper_iterations,\n",
    "                              imputer=None)\n",
    "\n",
    "print('\\nPerforming Bayesian search for hyper-parameters optimization, with missing data replaced with mean imputer')\n",
    "\n",
    "run_exp_bayes_hyperparams_opt(X_train,\n",
    "                              y_train,\n",
    "                              X_val,\n",
    "                              y_val,\n",
    "                              cat_features=cat_features,\n",
    "                              param_space=param_space,\n",
    "                              max_evals=hyper_iterations,\n",
    "                              imputer=mean_imputer)\n",
    "\n",
    "print(\n",
    "    '\\nPerforming Bayesian search for hyper-parameters optimization, with missing data replaced with iterative imputer')\n",
    "\n",
    "selected_model_imputed = run_exp_bayes_hyperparams_opt(X_train,\n",
    "                                                       y_train,\n",
    "                                                       X_val,\n",
    "                                                       y_val,\n",
    "                                                       cat_features=cat_features,\n",
    "                                                       param_space=param_space,\n",
    "                                                       max_evals=hyper_iterations,\n",
    "                                                       imputer=iter_imputer)\n",
    "\n",
    "print('\\nPerforming Bayesian search for hyper-parameters optimization, without replacement of missing data')\n",
    "\n",
    "selected_model = run_exp_bayes_hyperparams_opt(X_train,\n",
    "                                               y_train,\n",
    "                                               X_val,\n",
    "                                               y_val,\n",
    "                                               cat_features=cat_features,\n",
    "                                               param_space=param_space,\n",
    "                                               max_evals=hyper_iterations,\n",
    "                                               imputer=None)\n",
    "\n",
    "print('\\nPerforming Bayesian search for hyper-parameters optimization, without replacement and with weights')\n",
    "\n",
    "w, stats = compute_weights(y_train)\n",
    "print('Computed weights')\n",
    "print('For', stats['total_pos'], 'positive samples:', stats['pos_weight'])\n",
    "print('For', stats['total_neg'], 'negative samples:', stats['neg_weight'])\n",
    "\n",
    "run_exp_bayes_hyperparams_opt(X_train,\n",
    "                              y_train,\n",
    "                              X_val,\n",
    "                              y_val,\n",
    "                              cat_features=cat_features,\n",
    "                              param_space=param_space,\n",
    "                              max_evals=hyper_iterations,\n",
    "                              imputer=None,\n",
    "                              weights=w)\n",
    "\n",
    "''' Grid-search is done on the dev. set, as the grid-search takes care of splitting it into training and validation.\n",
    "Note: if `search_by_train_test_split` is set to True, every combination of values of the hyper-parameters is evaluated\n",
    "with a basic training/val. split of the dataset; if set to False, then every combination is evaluated with x-evaluation.\n",
    "Once method grid_search() has selected the best combination of hyper-parameters, fits a model with it. The final model \n",
    "can be evaluated with x-evaluation by setting parameter `calc_cv_statistics` to True (default). '''\n",
    "\n",
    "\n",
    "def run_exp_grid_hyperparams_opt(X, y, cat_features, seed, iterations, param_grid, cv_folds, imputer=None):\n",
    "    dev_pool, X_inputed = make_imputed_pool(X, y, imputer, cat_features)\n",
    "    model = CatBoostClassifier(iterations=iterations,\n",
    "                               eval_metric='AUC:hints=skip_train~false',\n",
    "                               cat_features=cat_features,\n",
    "                               random_state=seed)\n",
    "\n",
    "    grid_search_results = model.grid_search(X=dev_pool,\n",
    "                                            param_grid=param_grid,\n",
    "                                            search_by_train_test_split=True,\n",
    "                                            calc_cv_statistics=True,\n",
    "                                            cv=cv_folds,\n",
    "                                            partition_random_seed=seed,\n",
    "                                            verbose=True)\n",
    "\n",
    "    best_iter = np.argmax(grid_search_results['cv_results']['test-AUC-mean'])\n",
    "    best_AUC = grid_search_results['cv_results']['test-AUC-mean'][best_iter]\n",
    "    loss_for_best_AUC = grid_search_results['cv_results']['test-Logloss-mean'][best_iter]\n",
    "    print('Best params', grid_search_results['params'], 'with AUC', best_AUC, 'obtained at iteration', best_iter,\n",
    "          'with Log loss', loss_for_best_AUC)\n",
    "    return model\n",
    "\n",
    "\n",
    "param_grid = {'learning_rate': np.arange(.01, .2, .01),\n",
    "              'depth': [2, 3, 4, 5, 6, 7, 8]}\n",
    "\n",
    "print('\\nPerforming grid-search for hyper-parameters optimization while maintaining missing data')\n",
    "\n",
    "run_exp_grid_hyperparams_opt(X=X_dev,\n",
    "                             y=y_dev,\n",
    "                             cat_features=cat_features,\n",
    "                             seed=seed,\n",
    "                             iterations=iterations,\n",
    "                             param_grid=param_grid,\n",
    "                             cv_folds=cv_folds,\n",
    "                             imputer=None)\n",
    "\n",
    "# Cross-validate the two selected models, and test them on the test set\n",
    "for model, descr, imputer in zip((selected_model, selected_model_imputed), ('without imputation', 'with imputation'),\n",
    "                                 (None, iter_imputer)):\n",
    "    print(f'\\nCross-validating model {descr}.')\n",
    "    params = model.get_params()\n",
    "    params['loss_function'] = 'Logloss'\n",
    "    params['eval_metric'] = 'AUC:hints=skip_train~false'\n",
    "    X_pool, _ = make_imputed_pool(X_dev, y_dev, imputer=imputer, cat_features=cat_features, weight=None)\n",
    "    cv_results = cv(pool=X_pool,\n",
    "                    params=params,\n",
    "                    iterations=iterations,\n",
    "                    fold_count=4,\n",
    "                    partition_random_seed=seed,\n",
    "                    stratified=True,\n",
    "                    verbose=False)\n",
    "    # Find the iteration with the best test AUC, its AUC and other train and test stats.\n",
    "    best_cv_iter = np.argmax(cv_results['test-AUC-mean'])  # All the stats retrieved will refer to this same iteration\n",
    "    best_cv_val_AUC = cv_results['test-AUC-mean'][best_cv_iter]\n",
    "    best_cv_val_Logloss = cv_results['test-Logloss-mean'][best_cv_iter]\n",
    "    best_cv_train_AUC = cv_results['train-AUC-mean'][best_cv_iter]\n",
    "    best_cv_train_Logloss = cv_results['train-Logloss-mean'][best_cv_iter]\n",
    "    print('Best validation with parameters', params, 'achieved at iteration', best_cv_iter)\n",
    "    print(f'Training: Logloss {best_cv_train_Logloss}   ROC AUC {best_cv_train_AUC}')\n",
    "    print(f'Validation: Logloss {best_cv_val_Logloss}   ROC AUC {best_cv_val_AUC}')\n",
    "\n",
    "    print('Re-fitting the model and testing it')\n",
    "    test_pool, _ = make_imputed_pool(X_test, y_test, imputer=None, cat_features=cat_features, weight=None)\n",
    "    params['iterations'] = best_cv_iter + 1  # Shrink the model to the best iteration found during cross-validation\n",
    "    cv_model = CatBoostClassifier(**params)\n",
    "    training_res = cv_model.fit(X_pool, eval_set=test_pool, verbose=False)\n",
    "    print('Iteration:', training_res.best_iteration_)\n",
    "    y_train_preds = cv_model.predict_proba(X_dev)[:, 1]\n",
    "    training_AUC = roc_auc_score(y_dev, y_train_preds)\n",
    "    print(f\"Training (on dev. set): Log loss={training_res.best_score_['learn']['Logloss']}   ROC AUC={training_AUC}\")\n",
    "    print(\n",
    "        f\"Test (on test set): Log loss={training_res.best_score_['validation']['Logloss']}   ROC AUC={training_res.best_score_['validation']['AUC']}\")\n",
    "\n",
    "''' TODO\n",
    "Try Karpathy approach\n",
    "Leverage Tensorboard\n",
    "How to display CatBoost charts outside of notebook? Is it possible?\n",
    "Explore Seaborne\n",
    "Use the whole HANES dataset from CDC, and also try with GPU\n",
    "Try other strategies for imputation based on mean encoding and similar\n",
    "Instead of checking if survival after 10 years, estimate the number of years of survival\n",
    "C-index is the same as the ROC AUC for logistic regression.\n",
    "   see https://www.statisticshowto.com/c-statistic/#:~:text=A%20weighted%20c-index%20is,correctly%20predicting%20a%20negative%20outcome\n",
    "   and also https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4886856/  and https://bit.ly/3dvUh07\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
